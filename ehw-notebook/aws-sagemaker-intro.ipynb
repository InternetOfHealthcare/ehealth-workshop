{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 1, 'target': [100, 60, 97, 67, 73, 80, 69, 78, 76, 74, 98, 66, 61, 70, 100, 89, 85, 99, 64, 86, 81, 89, 82, 85, 70, 99, 88, 61, 80, 98, 65, 75, 98, 88, 99, 90, 98, 89, 78, 95, 80, 72, 89, 88, 61, 71, 94, 74, 67, 85, 62, 65, 85, 96, 85, 74, 72, 74, 65, 79, 91, 67, 64, 79, 63, 83, 97, 78, 62, 82, 99, 88, 83, 66, 77, 73, 60, 76, 60, 62, 92, 88, 78, 98, 64, 79, 88, 83, 87, 60, 82, 66, 87, 61, 80, 96, 91, 84, 72, 78, 73, 99, 79, 61, 94, 74, 84, 82, 74, 77, 84, 92, 76, 69, 86, 64, 91, 84, 92, 94], 'start': '2018-01-01 08:00:00'}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "import datetime \n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "time_fmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "minutes = 120\n",
    "start_time = datetime.datetime(2018, 1, 1, 8, 0)\n",
    "categories = 10\n",
    "dataset = []\n",
    "\n",
    "def mk_hrs(start_time):\n",
    "    hrs = []\n",
    "    for m in range(minutes):\n",
    "        hr = round(60 + (40 * random.random()))\n",
    "        hrs.append(hr)\n",
    "    return hrs\n",
    "\n",
    "for cat in range(categories):\n",
    "    hr_data = mk_hrs(start_time)\n",
    "    item = {\"start\": start_time.strftime(time_fmt), \"target\": hr_data , \"cat\": cat}\n",
    "    dataset.append(item)\n",
    "    \n",
    "print(dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(dataset)\n",
    "split = round(len(dataset) * 0.7)\n",
    "training = dataset[:split]\n",
    "test = dataset[split:]\n",
    "print (len(training))\n",
    "print (len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "uuid = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket         = 'hrdata_'+uuid\n",
    "prefix         = 'sagemaker'\n",
    "region         = \"us-east-1\" \n",
    "train_key      = 'deepar_training.json'\n",
    "test_key       = 'deepar_test.json'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "output_prefix  = '{}/{}'.format(prefix, 'output')\n",
    "output_path = 's3://{}/{}'.format(bucket, output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       7 deepar_training.json\n",
      "       3 deepar_test.json\n"
     ]
    }
   ],
   "source": [
    "def write_datafile(list,file):\n",
    "    fh = open(file, \"w\")\n",
    "    for data in list:\n",
    "        print(json.dumps(data),file=fh)\n",
    "    fh.close\n",
    "\n",
    "write_datafile(training,train_key)\n",
    "write_datafile(test,test_key)\n",
    "\n",
    "!wc -l {train_key}\n",
    "!wc -l {test_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cat\": 3, \"target\": [76, 61, 97, 92, 67, 99, 84, 73, 99, 77, 98, 67, 89, 93, 91, 75, 72, 62, 87, 88, 61, 62, 74, 90, 73, 79, 73, 85, 69, 89, 72, 67, 71, 85, 90, 89, 91, 68, 81, 76, 76, 94, 66, 93, 83, 71, 61, 99, 73, 93, 88, 94, 63, 69, 83, 99, 67, 76, 93, 96, 72, 83, 62, 69, 84, 96, 76, 91, 68, 70, 86, 100, 76, 89, 62, 66, 92, 73, 78, 80, 84, 85, 81, 74, 86, 62, 78, 77, 90, 76, 84, 68, 99, 65, 65, 87, 93, 99, 70, 91, 80, 88, 87, 97, 85, 90, 78, 71, 88, 63, 89, 88, 90, 81, 84, 80, 98, 90, 88, 71], \"start\": \"2018-01-01 08:00:00\"}\r\n",
      "{\"cat\": 8, \"target\": [75, 62, 69, 83, 91, 62, 61, 63, 80, 68, 76, 71, 65, 69, 84, 77, 64, 85, 93, 85, 85, 98, 79, 75, 76, 77, 70, 71, 96, 71, 80, 95, 92, 85, 97, 78, 68, 62, 76, 92, 61, 95, 67, 62, 69, 73, 83, 82, 93, 84, 99, 77, 82, 67, 63, 91, 82, 80, 78, 70, 89, 97, 71, 70, 77, 92, 80, 78, 75, 70, 87, 86, 69, 88, 99, 70, 72, 91, 80, 78, 91, 68, 81, 78, 65, 65, 98, 95, 65, 98, 88, 69, 73, 80, 91, 89, 67, 75, 72, 67, 83, 73, 81, 82, 78, 90, 63, 72, 62, 84, 81, 68, 73, 61, 93, 63, 86, 73, 88, 78], \"start\": \"2018-01-01 08:00:00\"}\r\n",
      "{\"cat\": 1, \"target\": [100, 60, 97, 67, 73, 80, 69, 78, 76, 74, 98, 66, 61, 70, 100, 89, 85, 99, 64, 86, 81, 89, 82, 85, 70, 99, 88, 61, 80, 98, 65, 75, 98, 88, 99, 90, 98, 89, 78, 95, 80, 72, 89, 88, 61, 71, 94, 74, 67, 85, 62, 65, 85, 96, 85, 74, 72, 74, 65, 79, 91, 67, 64, 79, 63, 83, 97, 78, 62, 82, 99, 88, 83, 66, 77, 73, 60, 76, 60, 62, 92, 88, 78, 98, 64, 79, 88, 83, 87, 60, 82, 66, 87, 61, 80, 96, 91, 84, 72, 78, 73, 99, 79, 61, 94, 74, 84, 82, 74, 77, 84, 92, 76, 69, 86, 64, 91, 84, 92, 94], \"start\": \"2018-01-01 08:00:00\"}\r\n",
      "{\"cat\": 9, \"target\": [90, 67, 83, 66, 82, 79, 82, 96, 78, 84, 73, 77, 84, 100, 74, 72, 87, 67, 91, 100, 85, 93, 67, 64, 99, 90, 79, 97, 87, 80, 99, 97, 70, 95, 64, 61, 72, 83, 94, 92, 88, 90, 93, 90, 76, 87, 88, 79, 97, 67, 83, 92, 97, 99, 82, 98, 64, 68, 97, 61, 63, 81, 99, 73, 67, 61, 76, 82, 96, 87, 99, 100, 68, 66, 92, 92, 72, 68, 79, 87, 95, 98, 84, 60, 67, 89, 65, 86, 96, 70, 73, 91, 90, 89, 97, 62, 88, 86, 72, 74, 88, 94, 92, 75, 88, 61, 85, 83, 64, 99, 71, 83, 70, 63, 70, 92, 69, 82, 82, 69], \"start\": \"2018-01-01 08:00:00\"}\r\n",
      "{\"cat\": 7, \"target\": [61, 98, 61, 75, 85, 90, 75, 91, 97, 73, 74, 63, 87, 99, 65, 95, 81, 78, 77, 96, 69, 73, 71, 77, 65, 83, 93, 92, 79, 98, 84, 84, 95, 87, 91, 74, 83, 85, 77, 95, 78, 99, 82, 97, 97, 75, 94, 70, 79, 78, 91, 68, 63, 75, 70, 70, 100, 76, 87, 72, 82, 72, 69, 81, 80, 61, 75, 93, 83, 63, 89, 71, 74, 67, 95, 96, 78, 60, 60, 78, 90, 86, 72, 61, 76, 75, 100, 70, 91, 72, 85, 61, 81, 70, 97, 79, 61, 67, 99, 80, 82, 90, 94, 72, 75, 65, 78, 95, 63, 75, 82, 68, 73, 77, 89, 84, 72, 62, 82, 64], \"start\": \"2018-01-01 08:00:00\"}\r\n",
      "{\"cat\": 6, \"target\": [78, 75, 89, 68, 77, 79, 71, 63, 76, 77, 80, 96, 70, 86, 75, 63, 78, 61, 61, 82, 86, 98, 89, 81, 99, 78, 69, 97, 87, 82, 90, 91, 85, 74, 80, 98, 80, 91, 90, 98, 66, 77, 60, 74, 90, 72, 69, 91, 86, 71, 68, 88, 74, 83, 99, 81, 74, 74, 92, 77, 62, 71, 79, 76, 86, 98, 86, 71, 99, 99, 97, 92, 63, 69, 83, 61, 96, 99, 75, 70, 71, 85, 88, 87, 92, 93, 78, 80, 87, 63, 64, 69, 81, 99, 90, 88, 77, 80, 95, 75, 87, 64, 65, 78, 75, 92, 94, 73, 71, 70, 73, 96, 63, 74, 91, 65, 85, 72, 87, 85], \"start\": \"2018-01-01 08:00:00\"}\r\n",
      "{\"cat\": 5, \"target\": [80, 86, 98, 94, 78, 89, 88, 90, 78, 92, 72, 73, 67, 71, 68, 77, 94, 91, 74, 73, 84, 97, 67, 84, 98, 70, 93, 87, 64, 67, 86, 63, 75, 87, 91, 95, 98, 92, 77, 86, 61, 65, 69, 71, 81, 76, 90, 78, 60, 85, 64, 66, 80, 92, 76, 61, 77, 87, 86, 80, 60, 67, 96, 67, 76, 99, 87, 87, 67, 88, 79, 93, 82, 97, 94, 70, 60, 68, 93, 83, 70, 84, 87, 63, 64, 81, 98, 88, 93, 86, 76, 89, 91, 71, 70, 88, 85, 84, 86, 90, 93, 66, 83, 65, 90, 98, 76, 71, 87, 77, 76, 84, 63, 88, 73, 71, 71, 95, 97, 62], \"start\": \"2018-01-01 08:00:00\"}\r\n"
     ]
    }
   ],
   "source": [
    "!head {train_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: hrdata_aa56b284-6ad1-4d9f-b5c4-e04c36ae2902\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 mb s3://{bucket}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path  = sagemaker_session.upload_data(train_key, bucket=bucket, key_prefix=train_prefix)\n",
    "test_path   = sagemaker_session.upload_data(test_key,  bucket=bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-04 17:50:11       1608 sagemaker/test/deepar_test.json\r\n",
      "2018-02-04 17:50:10       3753 sagemaker/train/deepar_training.json\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive s3://{bucket}/sagemaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containers = {\n",
    "    'us-east-1': '522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-east-2': '566113047672.dkr.ecr.us-east-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'us-west-2': '156387875391.dkr.ecr.us-west-2.amazonaws.com/forecasting-deepar:latest',\n",
    "    'eu-west-1': '224300973850.dkr.ecr.eu-west-1.amazonaws.com/forecasting-deepar:latest'\n",
    "}\n",
    "\n",
    "image_name = containers[region]\n",
    "image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::030555009967:role/sagemaker_role_aa56b284-6ad1-4d9f-b5c4-e04c36ae2902'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role_name=\"sagemaker_role_{}\".format(uuid)\n",
    "role_arn = !aws iam create-role \\\n",
    "    --role-name \"{role_name}\" \\\n",
    "    --assume-role-policy-document \"file://sagemaker-trust.json\" \\\n",
    "    --query Role.Arn \\\n",
    "    --output text\n",
    "    \n",
    "role_arn = role_arn.s\n",
    "role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws iam attach-role-policy \\\n",
    "    --role-name {role_name} \\\n",
    "    --policy-arn \"arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws iam attach-role-policy \\\n",
    "    --role-name {role_name} \\\n",
    "    --policy-arn \"arn:aws:iam::aws:policy/AmazonS3FullAccess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role_arn,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.8xlarge',\n",
    "    base_job_name='bp-job',\n",
    "    output_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_length = 30\n",
    "hyperparameters = {\n",
    "    \"time_freq\": 'M',\n",
    "    \"context_length\": prediction_length,\n",
    "    \"prediction_length\": prediction_length, # number of data points to predict\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"2\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"250\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.0001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\" # stop if loss hasn't improved in 10 epochs\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 's3://hrdata_aa56b284-6ad1-4d9f-b5c4-e04c36ae2902/sagemaker/test/deepar_test.json',\n",
       " 'train': 's3://hrdata_aa56b284-6ad1-4d9f-b5c4-e04c36ae2902/sagemaker/train/deepar_training.json'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_channels = {\"train\": train_path, \"test\": test_path}\n",
    "data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: bp-job-2018-02-04-16-51-44-527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:35 INFO 139784009750336] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'dropout_rate': u'0.05', u'cardinality': u'', u'test_quantiles': u'[0.5, 0.9]', u'_num_gpus': u'auto', u'learning_rate': u'0.001', u'_kvstore': u'auto', u'num_layers': u'3', u'embedding_dimension': u'', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'32', u'likelihood': u'gaussian', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:35 INFO 139784009750336] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'dropout_rate': u'0.05', u'learning_rate': u'0.0001', u'num_cells': u'40', u'prediction_length': u'30', u'epochs': u'250', u'time_freq': u'M', u'context_length': u'30', u'num_layers': u'2', u'mini_batch_size': u'32', u'likelihood': u'gaussian', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:35 INFO 139784009750336] Final configuration: {u'dropout_rate': u'0.05', u'test_quantiles': u'[0.5, 0.9]', u'learning_rate': u'0.0001', u'num_cells': u'40', u'epochs': u'250', u'embedding_dimension': u'', u'num_layers': u'2', u'_num_kv_servers': u'auto', u'cardinality': u'', u'likelihood': u'gaussian', u'mini_batch_size': u'32', u'_num_gpus': u'auto', u'prediction_length': u'30', u'time_freq': u'M', u'context_length': u'30', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:35 INFO 139784009750336] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] nvidia-smi took: 0.0251650810242 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Calculating training set statistics\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 WARNING 139784009750336] Dataset contains very few time-series which may not be sufficient for good accuracy\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Training set statistics:\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Integer timeseries\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] number of timeseries: 7\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] number of observations: 840\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] mean target length: 120\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] min/mean/max target: 60.0/80.152381897/100.0\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Small number of time-series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Calculating training set statistics\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Test set statistics:\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Integer timeseries\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] number of timeseries: 3\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] number of observations: 360\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] mean target length: 120\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] min/mean/max target: 60.0/80.6138916016/100.0\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 104.10785675048828, \"sum\": 104.10785675048828, \"min\": 104.10785675048828}}, \"EndTime\": 1517763457.237604, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763457.13268}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 242.54798889160156, \"sum\": 242.54798889160156, \"min\": 242.54798889160156}}, \"EndTime\": 1517763457.375277, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763457.237651}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:37 INFO 139784009750336] Epoch[0] Batch[0] avg_epoch_loss=116.608788\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:38 INFO 139784009750336] Epoch[0] Batch[5] avg_epoch_loss=39.840409\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:38 INFO 139784009750336] Epoch[0] Batch [5]#011Speed: 311.18 samples/sec#011loss=39.840409\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:38 INFO 139784009750336] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 250, \"sum\": 250.0, \"min\": 250}, \"update.time\": {\"count\": 1, \"max\": 1195.8518028259277, \"sum\": 1195.8518028259277, \"min\": 1195.8518028259277}}, \"EndTime\": 1517763458.586044, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763457.375328}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:38 INFO 139784009750336] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:38 INFO 139784009750336] Saved checkpoint to \"/opt/ml/model/state_9ca8abfb-a0b1-47e4-b9b3-ec3b0de20958-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 85.35194396972656, \"sum\": 85.35194396972656, \"min\": 85.35194396972656}}, \"EndTime\": 1517763458.671641, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763458.586119}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:38 INFO 139784009750336] Epoch[1] Batch[0] avg_epoch_loss=85.417328\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:39 INFO 139784009750336] Epoch[1] Batch[5] avg_epoch_loss=32.451460\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:39 INFO 139784009750336] Epoch[1] Batch [5]#011Speed: 317.86 samples/sec#011loss=32.451460\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:39 INFO 139784009750336] Epoch[1] Batch[10] avg_epoch_loss=35.292453\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:39 INFO 139784009750336] Epoch[1] Batch [10]#011Speed: 274.88 samples/sec#011loss=38.701645\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:39 INFO 139784009750336] processed a total of 352 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1215.2950763702393, \"sum\": 1215.2950763702393, \"min\": 1215.2950763702393}}, \"EndTime\": 1517763459.898582, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763458.671699}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:39 INFO 139784009750336] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:39 INFO 139784009750336] Saved checkpoint to \"/opt/ml/model/state_bc68d395-24c9-4afd-8829-b5190d90d753-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 15.90108871459961, \"sum\": 15.90108871459961, \"min\": 15.90108871459961}}, \"EndTime\": 1517763459.914683, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763459.898633}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:39 INFO 139784009750336] Epoch[2] Batch[0] avg_epoch_loss=100.386436\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:40 INFO 139784009750336] Epoch[2] Batch[5] avg_epoch_loss=74.364562\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:40 INFO 139784009750336] Epoch[2] Batch [5]#011Speed: 271.10 samples/sec#011loss=74.364562\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:41 INFO 139784009750336] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1108.8027954101562, \"sum\": 1108.8027954101562, \"min\": 1108.8027954101562}}, \"EndTime\": 1517763461.03519, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763459.914727}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:41 INFO 139784009750336] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:41 INFO 139784009750336] Epoch[3] Batch[0] avg_epoch_loss=5.288023\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:41 INFO 139784009750336] Epoch[3] Batch[5] avg_epoch_loss=17.328727\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:41 INFO 139784009750336] Epoch[3] Batch [5]#011Speed: 277.65 samples/sec#011loss=17.328727\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:42 INFO 139784009750336] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1087.3441696166992, \"sum\": 1087.3441696166992, \"min\": 1087.3441696166992}}, \"EndTime\": 1517763462.134252, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763461.035254}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:42 INFO 139784009750336] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:42 INFO 139784009750336] Epoch[4] Batch[0] avg_epoch_loss=5.278403\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:42 INFO 139784009750336] Epoch[4] Batch[5] avg_epoch_loss=15.339036\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:42 INFO 139784009750336] Epoch[4] Batch [5]#011Speed: 258.37 samples/sec#011loss=15.339036\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:43 INFO 139784009750336] Epoch[4] Batch[10] avg_epoch_loss=10.758422\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:43 INFO 139784009750336] Epoch[4] Batch [10]#011Speed: 343.61 samples/sec#011loss=5.261686\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:43 INFO 139784009750336] processed a total of 352 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1134.1328620910645, \"sum\": 1134.1328620910645, \"min\": 1134.1328620910645}}, \"EndTime\": 1517763463.279986, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763462.134328}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:43 INFO 139784009750336] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:43 INFO 139784009750336] Saved checkpoint to \"/opt/ml/model/state_76514bb8-0bfe-4a48-a43a-263c4d590aeb-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 58.80022048950195, \"sum\": 58.80022048950195, \"min\": 58.80022048950195}}, \"EndTime\": 1517763463.339016, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763463.280053}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:43 INFO 139784009750336] Epoch[5] Batch[0] avg_epoch_loss=5.264173\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:43 INFO 139784009750336] Epoch[5] Batch[5] avg_epoch_loss=25.986039\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:43 INFO 139784009750336] Epoch[5] Batch [5]#011Speed: 307.14 samples/sec#011loss=25.986039\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:44 INFO 139784009750336] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1112.9601001739502, \"sum\": 1112.9601001739502, \"min\": 1112.9601001739502}}, \"EndTime\": 1517763464.463658, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763463.339097}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:44 INFO 139784009750336] loss did not improve for 1 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:44 INFO 139784009750336] Epoch[6] Batch[0] avg_epoch_loss=5.231376\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:45 INFO 139784009750336] Epoch[6] Batch[5] avg_epoch_loss=14.846060\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:45 INFO 139784009750336] Epoch[6] Batch [5]#011Speed: 301.41 samples/sec#011loss=14.846060\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:45 INFO 139784009750336] processed a total of 288 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1047.455072402954, \"sum\": 1047.455072402954, \"min\": 1047.455072402954}}, \"EndTime\": 1517763465.526451, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763464.463721}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:45 INFO 139784009750336] loss did not improve for 2 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:45 INFO 139784009750336] Epoch[7] Batch[0] avg_epoch_loss=69.172066\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:46 INFO 139784009750336] Epoch[7] Batch[5] avg_epoch_loss=15.887937\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:46 INFO 139784009750336] Epoch[7] Batch [5]#011Speed: 252.17 samples/sec#011loss=15.887937\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[02/04/2018 16:57:46 INFO 139784009750336] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1139.9478912353516, \"sum\": 1139.9478912353516, \"min\": 1139.9478912353516}}, \"EndTime\": 1517763466.679978, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763465.526533}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:46 INFO 139784009750336] loss did not improve for 3 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:46 INFO 139784009750336] Epoch[8] Batch[0] avg_epoch_loss=5.201490\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:47 INFO 139784009750336] Epoch[8] Batch[5] avg_epoch_loss=23.701459\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:47 INFO 139784009750336] Epoch[8] Batch [5]#011Speed: 256.55 samples/sec#011loss=23.701459\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:47 INFO 139784009750336] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1116.1890029907227, \"sum\": 1116.1890029907227, \"min\": 1116.1890029907227}}, \"EndTime\": 1517763467.810856, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763466.680065}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:47 INFO 139784009750336] loss did not improve for 4 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:47 INFO 139784009750336] Epoch[9] Batch[0] avg_epoch_loss=56.681221\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:48 INFO 139784009750336] Epoch[9] Batch[5] avg_epoch_loss=30.063378\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:48 INFO 139784009750336] Epoch[9] Batch [5]#011Speed: 268.13 samples/sec#011loss=30.063378\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:49 INFO 139784009750336] Epoch[9] Batch[10] avg_epoch_loss=23.399521\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:49 INFO 139784009750336] Epoch[9] Batch [10]#011Speed: 302.41 samples/sec#011loss=15.402894\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:49 INFO 139784009750336] processed a total of 352 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1183.628797531128, \"sum\": 1183.628797531128, \"min\": 1183.628797531128}}, \"EndTime\": 1517763469.006312, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763467.81092}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:49 INFO 139784009750336] loss did not improve for 5 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:49 INFO 139784009750336] Epoch[10] Batch[0] avg_epoch_loss=5.166333\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:49 INFO 139784009750336] Epoch[10] Batch[5] avg_epoch_loss=37.617106\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:49 INFO 139784009750336] Epoch[10] Batch [5]#011Speed: 315.13 samples/sec#011loss=37.617106\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:50 INFO 139784009750336] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1097.3811149597168, \"sum\": 1097.3811149597168, \"min\": 1097.3811149597168}}, \"EndTime\": 1517763470.11813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763469.006373}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:50 INFO 139784009750336] loss did not improve for 6 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:50 INFO 139784009750336] Epoch[11] Batch[0] avg_epoch_loss=47.454048\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:50 INFO 139784009750336] Epoch[11] Batch[5] avg_epoch_loss=18.142189\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:50 INFO 139784009750336] Epoch[11] Batch [5]#011Speed: 292.47 samples/sec#011loss=18.142189\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:51 INFO 139784009750336] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1156.3420295715332, \"sum\": 1156.3420295715332, \"min\": 1156.3420295715332}}, \"EndTime\": 1517763471.289399, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763470.118205}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:51 INFO 139784009750336] loss did not improve for 7 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:51 INFO 139784009750336] Epoch[12] Batch[0] avg_epoch_loss=5.122880\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:51 INFO 139784009750336] Epoch[12] Batch[5] avg_epoch_loss=11.330452\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:51 INFO 139784009750336] Epoch[12] Batch [5]#011Speed: 343.18 samples/sec#011loss=11.330452\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:52 INFO 139784009750336] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1019.819974899292, \"sum\": 1019.819974899292, \"min\": 1019.819974899292}}, \"EndTime\": 1517763472.321323, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763471.289449}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:52 INFO 139784009750336] loss did not improve for 8 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:52 INFO 139784009750336] Epoch[13] Batch[0] avg_epoch_loss=42.754074\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:52 INFO 139784009750336] Epoch[13] Batch[5] avg_epoch_loss=22.452957\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:52 INFO 139784009750336] Epoch[13] Batch [5]#011Speed: 329.18 samples/sec#011loss=22.452957\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:53 INFO 139784009750336] processed a total of 320 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1040.390968322754, \"sum\": 1040.390968322754, \"min\": 1040.390968322754}}, \"EndTime\": 1517763473.375036, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763472.321378}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:53 INFO 139784009750336] loss did not improve for 9 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:53 INFO 139784009750336] Epoch[14] Batch[0] avg_epoch_loss=41.013447\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:54 INFO 139784009750336] Epoch[14] Batch[5] avg_epoch_loss=16.566707\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:54 INFO 139784009750336] Epoch[14] Batch [5]#011Speed: 299.16 samples/sec#011loss=16.566707\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:54 INFO 139784009750336] Epoch[14] Batch[10] avg_epoch_loss=20.540041\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:54 INFO 139784009750336] Epoch[14] Batch [10]#011Speed: 260.44 samples/sec#011loss=25.308043\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:54 INFO 139784009750336] processed a total of 352 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1282.9549312591553, \"sum\": 1282.9549312591553, \"min\": 1282.9549312591553}}, \"EndTime\": 1517763474.672847, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763473.375096}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:54 INFO 139784009750336] loss did not improve for 10 epochs\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:54 INFO 139784009750336] stopping training now\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:54 INFO 139784009750336] Loading parameters from best epoch (4)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 7.03883171081543, \"sum\": 7.03883171081543, \"min\": 7.03883171081543}}, \"EndTime\": 1517763474.680135, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763474.672895}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:54 INFO 139784009750336] Final loss: 10.7584222013 (occured at epoch 4)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 944.0720081329346, \"sum\": 944.0720081329346, \"min\": 944.0720081329346}}, \"EndTime\": 1517763475.624376, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763474.680174}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:56 INFO 139784009750336] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 1585.2549076080322, \"sum\": 1585.2549076080322, \"min\": 1585.2549076080322}}, \"EndTime\": 1517763476.265537, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763475.624442}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:56 INFO 139784009750336] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:56 INFO 139784009750336] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 116.79315567016602, \"sum\": 116.79315567016602, \"min\": 116.79315567016602}}, \"EndTime\": 1517763476.382417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763476.265586}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:56 INFO 139784009750336] Rebinding module with batch size 32 \u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:57 INFO 139784009750336] Executor bound\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 849.0259647369385, \"sum\": 849.0259647369385, \"min\": 849.0259647369385}}, \"EndTime\": 1517763477.232396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763476.382462}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 1198.984146118164, \"sum\": 1198.984146118164, \"min\": 1198.984146118164}}, \"EndTime\": 1517763477.582342, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763477.232788}\n",
      "\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:57 INFO 139784009750336] #test_score (algo-1, wQuantileLoss[0.5]): 0.359303\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:57 INFO 139784009750336] #test_score (algo-1, wQuantileLoss[0.9]): 0.274638\u001b[0m\n",
      "\u001b[31m[02/04/2018 16:57:57 INFO 139784009750336] #test_score (algo-1, RMSE): 26.1673491889\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 22360.51106452942, \"sum\": 22360.51106452942, \"min\": 22360.51106452942}, \"setuptime\": {\"count\": 1, \"max\": 12.272119522094727, \"sum\": 12.272119522094727, \"min\": 12.272119522094727}}, \"EndTime\": 1517763477.76149, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1517763477.582411}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bp-job-2018-02-04-16-51-44-527'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: bp-job-2018-02-04-16-51-44-527\n",
      "INFO:sagemaker:Creating endpoint-config with name bp-job-2018-02-04-16-51-44-527\n",
      "INFO:sagemaker:Creating endpoint with name bp-job-2018-02-04-16-51-44-527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1 = '0.1'         # compute p10 quantile\n",
    "q2 = '0.9'         # compute p90 quantile\n",
    "num_samples = 5    # predict 6 sample series\n",
    "target = test[0]\n",
    "\n",
    "s = {\"start\": \"2017-02-01 08:00:00\", \"target\": target}\n",
    "series = []\n",
    "series.append(s)\n",
    "\n",
    "configuration = {\n",
    "        \"output_types\": [\"samples\"],\n",
    "        \"num_samples\": num_samples}\n",
    "http_data = {\n",
    "        \"instances\": series, \n",
    "        \"configuration\": configuration\n",
    "}\n",
    "http_data = json.dumps(http_data)\n",
    "print(http_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
